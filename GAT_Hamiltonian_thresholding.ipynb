{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe60mzovU3Q8+zAi3mFGmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rizveeredwan/Annoying-Mute-Line/blob/master/GAT_Hamiltonian_thresholding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "THXE0dH-29c9"
      },
      "outputs": [],
      "source": [
        "# https://nn.labml.ai/graphs/gat/index.html\n",
        "# https://theaisummer.com/gnn-architectures/#:~:text=The%20main%20idea%20behind%20GAT,determine%20each%20node's%20%E2%80%9Cimportance%E2%80%9D."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install labml_helpers\n",
        "!pip install pytorchtools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oIoQFFI6QPv",
        "outputId": "573e9a1c-fc21-4cbe-b475-0a1cd6e96bc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: labml_helpers in /usr/local/lib/python3.9/dist-packages (0.4.89)\n",
            "Requirement already satisfied: labml>=0.4.158 in /usr/local/lib/python3.9/dist-packages (from labml_helpers) (0.4.161)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from labml_helpers) (1.13.1+cu116)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from labml>=0.4.158->labml_helpers) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from labml>=0.4.158->labml_helpers) (1.22.4)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.9/dist-packages (from labml>=0.4.158->labml_helpers) (3.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->labml_helpers) (4.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from gitpython->labml>=0.4.158->labml_helpers) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.158->labml_helpers) (5.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorchtools in /usr/local/lib/python3.9/dist-packages (0.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from labml_helpers.module import Module\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "from math import ceil, sqrt\n",
        "import csv"
      ],
      "metadata": {
        "id": "qKWkEi705tFt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8kQeGk8FpEWI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "1JT7cRgyVE52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d601b433-d4ba-4b22-bae2-de8eda50be94"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience):\n",
        "    self.loss = None\n",
        "    self.cnt = 0 \n",
        "    self.patience = patience\n",
        "  def check(self, curr_loss):\n",
        "    if self.loss is None:\n",
        "      self.loss = curr_loss\n",
        "      self.cnt = 0 \n",
        "    elif self.loss > curr_loss:\n",
        "      self.loss = curr_loss \n",
        "      self.cnt = 0 \n",
        "    elif self.loss <= curr_loss:\n",
        "      self.cnt += 1 \n",
        "    if self.cnt > self.patience:\n",
        "      return True \n",
        "    return False "
      ],
      "metadata": {
        "id": "yQVlxeJnr2Pl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttentionLayer(Module):\n",
        "  def __init__(self, in_features: int, out_features: int, n_heads: int, is_concat: bool = True, dropout: float = 0.6, leaky_relu_negative_slope: float = 0.2):\n",
        "    super().__init__()\n",
        "    self.is_concat = is_concat\n",
        "    self.n_heads = n_heads\n",
        "    if is_concat:\n",
        "      assert out_features % n_heads == 0\n",
        "      self.n_hidden = out_features // n_heads\n",
        "    else:\n",
        "      self.n_hidden = out_features\n",
        "    self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
        "    self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
        "    self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.dropout = nn.Dropout(dropout)    \n",
        "  def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
        "    n_nodes = h.shape[0]\n",
        "    g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
        "    g_repeat = g.repeat(n_nodes, 1, 1)\n",
        "    g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
        "    g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n",
        "    g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
        "    e = self.activation(self.attn(g_concat))\n",
        "    e = e.squeeze(-1)\n",
        "    assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
        "    assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
        "    assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
        "    e = e.masked_fill(adj_mat == 0, float('-inf')) \n",
        "    a = self.softmax(e)\n",
        "    a = self.dropout(a)\n",
        "    attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
        "    if self.is_concat:\n",
        "      return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
        "    else:\n",
        "       return attn_res.mean(dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, C, d, dh):\n",
        "    super().__init__()\n",
        "    self.C = C \n",
        "    self.dh = dh\n",
        "    self.layer1 = nn.Linear(d, dh, bias=False)\n",
        "    self.layer2 = nn.Linear(d, dh, bias=False)\n",
        "    self.alpha = nn.Tanh()\n",
        "    self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "  def only_neighbour_consideration(self, V, chosen, adj_list, label_map, epoch):\n",
        "    for node in range(len(chosen)-1, -1, -1):\n",
        "      o1 = self.layer1(V[chosen[node]])\n",
        "      _list = []\n",
        "      _idx = []\n",
        "      for j in range(len(adj_list[chosen[node]])):\n",
        "        v = adj_list[chosen[node]][j]\n",
        "        if label_map[v] is None: # calculation with each neighbour \n",
        "          o2 = self.layer2(V[v])\n",
        "          res = torch.dot(o1, o2) / sqrt(self.dh)\n",
        "          val = self.C * self.alpha(res)\n",
        "          _list.append(val)\n",
        "          non_zero = True \n",
        "          _idx.append(v)\n",
        "      if len(_list) > 0:\n",
        "        t = torch.tensor(_list)\n",
        "        res = self.softmax(t)\n",
        "        # print(\"node \", node, epoch, _list)\n",
        "        selected_node = torch.argmax(res)\n",
        "        return res[selected_node], _idx[selected_node] # corresponding_probability , selected_node \n",
        "    return None, None \n",
        "\n",
        "        \n",
        "\n",
        "  def forward(self, V, adj_mat, adj_list, chosen, label_map, current_epoch):\n",
        "    # print(o1.shape)\n",
        "    # print(o1)\n",
        "    temp = torch.squeeze(adj_mat)\n",
        "    decoder_output, best_node = self.only_neighbour_consideration(V=V, chosen=chosen, adj_list=adj_list, label_map=label_map, epoch=current_epoch)\n",
        "    return decoder_output, best_node \n",
        "    \"\"\"\n",
        "    _list = []\n",
        "    non_zero = None \n",
        "    for node in range(len(chosen)-1, -1, -1):\n",
        "      o1 = self.layer1(V[node])\n",
        "      _list = []\n",
        "      non_zero = None \n",
        "      for j in range(temp.shape[1]):\n",
        "        if temp[node][j] == 1: # calculation with each neighbour \n",
        "          o2 = self.layer2(V[j])\n",
        "          res = torch.dot(o1, o2) / sqrt(self.dh)\n",
        "          val = self.C * self.alpha(res)\n",
        "          _list.append(val)\n",
        "          non_zero = True \n",
        "        else:\n",
        "          _list.append(0) # not neighbour \n",
        "      if non_zero is not None:\n",
        "        t = torch.tensor(_list)\n",
        "        res = self.softmax(t)\n",
        "        print(\"node \", node, len(res), current_epoch, _list)\n",
        "        decoder_output, best_node = None, None \n",
        "        for j in range(0, res.shape[0]):\n",
        "          if label_map[j] is None: # from that which is not chosen \n",
        "            if decoder_output is None:\n",
        "              decoder_output = res[j] \n",
        "              best_node = j \n",
        "            if decoder_output < res[j]:\n",
        "              decoder_output = res[j] \n",
        "              best_node = j \n",
        "        if best_node is not None: # solution found \n",
        "          return decoder_output, best_node   \n",
        "    # chosen did not work: random distance neighbour pick up  \n",
        "    _list = []\n",
        "    o1 = self.layer1(V[len(chosen)-1])\n",
        "    for i in range(0, temp.shape[1]):\n",
        "      o2 = self.layer2(V[i])\n",
        "      res = torch.dot(o1, o2) / sqrt(self.dh)\n",
        "      val = self.C * self.alpha(res)\n",
        "      _list.append(val)\n",
        "    t = torch.tensor(_list)\n",
        "    res = self.softmax(t)\n",
        "    decoder_output, best_node = None, None \n",
        "    for j in range(0, res.shape[0]):\n",
        "      if label_map[j] is None:\n",
        "        if decoder_output is None:\n",
        "          decoder_output = res[j] \n",
        "          best_node = j \n",
        "        if decoder_output < res[j]:\n",
        "          decoder_output = res[j] \n",
        "          best_node = j \n",
        "    return decoder_output, best_node     \n",
        "  \"\"\"\n",
        "         \n",
        "   \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, n_nodes: int, in_features: int, out_features: int):\n",
        "    super().__init__()\n",
        "    self.n_nodes = n_nodes \n",
        "    self.embedding = nn.Embedding(n_nodes, in_features)\n",
        "    self.encoder1 = GraphAttentionLayer(in_features=in_features, out_features=out_features, n_heads=1, is_concat= True)\n",
        "    self.encoder2 = GraphAttentionLayer(in_features=out_features, out_features=out_features, n_heads=1, is_concat= True)\n",
        "    self.encoder3 = GraphAttentionLayer(in_features=out_features, out_features=out_features, n_heads=1, is_concat= True)\n",
        "\n",
        "    self.last_layer = nn.Linear(out_features, 1, bias=False)\n",
        "    self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "  def forward(self, adj_mat):\n",
        "    t = [i for i in range(self.n_nodes)]\n",
        "    t = torch.tensor(t)\n",
        "    em = self.embedding(t)\n",
        "    #print(em)\n",
        "    en1 = self.encoder1(em, adj_mat) \n",
        "    #print(\"en1 \", en1)\n",
        "    en2 = self.encoder2(en1, adj_mat) \n",
        "    #print(\"en2 \", en2)\n",
        "    en3 = self.encoder3(en2, adj_mat) \n",
        "    #print(\"en3 \", en3.shape, en3)\n",
        "    ll = self.last_layer(en3)\n",
        "    softmax = self.softmax(ll)\n",
        "    #print(softmax, softmax.shape)\n",
        "    idx = torch.argmax(softmax, dim=0).squeeze()\n",
        "    #print(idx)\n",
        "    return int(idx), en3, softmax[idx]\n",
        "  \n",
        " \n",
        "\n",
        "m = nn.Softmax(dim=2)\n",
        "input = torch.randn(2, 3, 4)\n",
        "#print(input)\n",
        "output = m(input)\n",
        "#print(output)"
      ],
      "metadata": {
        "id": "JhFXP4qp5WDm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def constraint_violation(label_maps, adj_mat):\n",
        "  # function to see how many violations have been observed \n",
        "  violation = 0\n",
        "  #print(label_maps)\n",
        "  print(adj_mat.shape)\n",
        "  temp = torch.squeeze(adj_mat)\n",
        "  print(temp.shape)\n",
        "  print(temp)\n",
        "  edge_count = 0 \n",
        "  for i in range(0, temp.shape[0]):\n",
        "    for j in range(i+1, temp.shape[0]):\n",
        "      if temp[i][j] == 1 and label_maps[i] == label_maps[j]:\n",
        "        violation += 1 \n",
        "      if temp[i][j] == 1:\n",
        "        #print(\"edge \", i,j )\n",
        "        edge_count += 1\n",
        "    # print(f\"{i} = {edge_count}\")\n",
        "  # print(\"edge count \",edge_count)\n",
        "  return violation "
      ],
      "metadata": {
        "id": "-vNKj-cRo68A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_reward(adj_mat, node_idx, prob, threshold, label_maps, initiate=False):\n",
        "  assert(adj_mat.shape[2] == 1) # n_heads == 1 \n",
        "  val = 0\n",
        "  cnt = 0\n",
        "  for i in range(0, adj_mat.shape[0]):\n",
        "    if adj_mat[node_idx][i][0] == 1: # adjacency \n",
        "      cnt += 1 \n",
        "  if prob>= threshold: # labeled as 1 \n",
        "    label_maps[node_idx] = 1\n",
        "  else:\n",
        "    if initiate==True:\n",
        "      label_maps[node_idx] = 1\n",
        "    else:\n",
        "      label_maps[node_idx] = 0\n",
        "  val = label_maps[node_idx] * (-cnt) * label_maps[node_idx] \n",
        "  # who are bigger\n",
        "  for j in range(node_idx+1, adj_mat.shape[0]): # i < j \n",
        "    #print(\"one \", label_maps)\n",
        "    if adj_mat[node_idx][j][0] == 1 and adj_mat[j][node_idx][0] == 1 and label_maps[j] is not None: # \n",
        "      val = val + label_maps[node_idx] * 2 * label_maps[j]\n",
        "  # who are smaller \n",
        "  for j in range(0, node_idx):\n",
        "    #print(\"two \", label_maps)\n",
        "    if adj_mat[node_idx][j][0] == 1 and adj_mat[j][node_idx][0] == 1 and label_maps[j] is not None:\n",
        "      val = val + label_maps[j] * 2 * label_maps[node_idx]\n",
        "  return val, label_maps \n",
        "\n",
        "def check_grad(model):\n",
        "  params = list(model.parameters())\n",
        "  print(\"length \", len(params))\n",
        "  for i in range(0, len(params)):\n",
        "    print(params[i].shape, params[i].requires_grad)\n",
        "    print(params[i].grad)\n",
        "\n",
        "def run(adj_mat, adj_list, epoch, threshold, d0=2, d1=5, d2=7, fig_name=\"50_499.jpg\", data_file_name=\"data.csv\", patience=20):\n",
        "  n = adj_mat.shape[0]\n",
        "  if n>=100000:\n",
        "    d0 = max(ceil(sqrt(n*1.0)), 2)\n",
        "  else:\n",
        "    # d0 = max(ceil(n**(1.0/3.0)), 2)\n",
        "    d0 = max(ceil(sqrt(n*1.0)), 2)\n",
        "  d1 = max(ceil(d0/2.0), 2)\n",
        "  d2 = max(2, ceil(d1/2.0))\n",
        "  print(f\"do = {d0} d1={d1} d2={d2}\")\n",
        "  encoder = Encoder(n_nodes = adj_mat.shape[0], in_features= d0, out_features=d1) \n",
        "  t = list(encoder.parameters())\n",
        "  # print(list(encoder.parameters()))\n",
        "  decoder = Decoder(C=10, d=d1, dh=ceil(d1/2.0))\n",
        "  # print(len(list(decoder.parameters())))\n",
        "\n",
        "  encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0001)\n",
        "  decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0001)\n",
        "  # initialize the early_stopping object\n",
        "  es = EarlyStopping(patience=patience)\n",
        "\n",
        "  save_loss = []\n",
        "  save_reward = []\n",
        "  save_result = []\n",
        "\n",
        "  for i in range(0, epoch):\n",
        "    print(\"epoch \", i)\n",
        "    first_node, encoded, prob = encoder(adj_mat)\n",
        "    # print(\"enoded \", encoded)\n",
        "    chosen =  []\n",
        "    label_maps = []\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    for j in range(0, adj_mat.shape[0]):\n",
        "      label_maps.append(None) \n",
        "    chosen.append(first_node)\n",
        "    #print(first_node, encoded, prob)\n",
        "    #print(\"prob \", prob)\n",
        "    reward, label_maps = calculate_reward(adj_mat, chosen[0], prob, threshold, label_maps, True)\n",
        "    loss = reward * prob \n",
        "    # print(\"reward \", reward)\n",
        "    for j in range(1, adj_mat.shape[0]):\n",
        "      decoder_output, best_node = decoder(encoded, adj_mat, adj_list, chosen, label_maps, i)\n",
        "      # print(\"j = \", j, best_node, decoder_output)\n",
        "      # print(\"came \", decoder_output, best_node, decoder_output.requires_grad)\n",
        "      chosen.append(best_node)\n",
        "      temp_reward, label_maps = calculate_reward(adj_mat, chosen[-1], decoder_output, threshold, label_maps, False)\n",
        "      reward = reward + temp_reward\n",
        "      loss = loss + reward * decoder_output\n",
        "      # print(\"loss \",loss, reward, best_node, label_maps, decoder_output)\n",
        "    loss.backward()\n",
        "    save_loss.append(loss.detach().numpy())\n",
        "    save_reward.append(reward)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    #check_grad(encoder)\n",
        "    #check_grad(decoder)\n",
        "    # print(f\"redward = {reward} loss = {loss}\")\n",
        "    #print(chosen) \n",
        "    violation = constraint_violation(label_maps, adj_mat)\n",
        "    print(f\"redward = {reward} loss = {loss} violation = {violation}\")\n",
        "    print(label_maps)\n",
        "    save_result.append((loss.detach().numpy(), violation))\n",
        "    # early_stopping needs the validation loss to check if it has decresed, \n",
        "    if es.check(curr_loss=loss) is True:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "  # save_loss.sort(key=lambda x:x[0], reverse=True)\n",
        "  save_result.sort(key=lambda x:x[1])\n",
        "  global MIN_VALIDATION_VIOLATION_OBSERVED\n",
        "  print(f\"MIN_VALIDATION_VIOLATION_OBSERVED = {MIN_VALIDATION_VIOLATION_OBSERVED}\")\n",
        "  if MIN_VALIDATION_VIOLATION_OBSERVED is None:\n",
        "    MIN_VALIDATION_VIOLATION_OBSERVED = save_result[0][1]\n",
        "    function_plot(data=save_loss, title='loss variation', ylabel='loss', xlabel='epoch', loc=\"upper left\", fig_name=fig_name)\n",
        "    function_plot(data=save_reward, title='reward variation', ylabel='reward', xlabel='epoch', loc=\"upper left\", fig_name=None)\n",
        "    write_into_csv(file_name=data_file_name, save_result=save_result, trim=30)  \n",
        "  elif MIN_VALIDATION_VIOLATION_OBSERVED > save_result[0][1]:\n",
        "    function_plot(data=save_loss, title='loss variation', ylabel='loss', xlabel='epoch', loc=\"upper left\", fig_name=fig_name)\n",
        "    function_plot(data=save_reward, title='reward variation', ylabel='reward', xlabel='epoch', loc=\"upper left\", fig_name=None)\n",
        "    write_into_csv(file_name=data_file_name, save_result=save_result, trim=30)  \n",
        "    MIN_VALIDATION_VIOLATION_OBSERVED = save_result[0][1]\n",
        "  else:\n",
        "    function_plot(data=save_loss, title='loss variation', ylabel='loss', xlabel='epoch', loc=\"upper left\", fig_name=None)\n",
        "    function_plot(data=save_reward, title='reward variation', ylabel='reward', xlabel='epoch', loc=\"upper left\", fig_name=None)\n",
        "  print(save_result[0:min(30, len(save_result))])"
      ],
      "metadata": {
        "id": "c6L5jk6z3Stc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_read(file_name=os.path.join('/content', \"gdrive\", 'MyDrive', 'Research', 'MADRL', 'Graph Dataset', \"50_499.txt\")):\n",
        "  # reading the text file \n",
        "  if os.path.exists(file_name):\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "      lines = f.readline().strip().split(' ')\n",
        "      n, e = int(lines[0]), int(lines[1])\n",
        "      adj_mat = [] \n",
        "      adj_list = []\n",
        "      for i in range(0, n):\n",
        "        adj_mat.append([])\n",
        "        adj_list.append([])\n",
        "        for j in range(0, n):\n",
        "          adj_mat[i].append(0)\n",
        "      for i in range(0, e):\n",
        "        lines = f.readline().strip().split(' ')\n",
        "        a, b = int(lines[0]), int(lines[1]) \n",
        "        adj_mat[a-1][b-1]=1\n",
        "        adj_mat[b-1][a-1]=1\n",
        "        adj_list[a-1].append(b-1)\n",
        "        adj_list[b-1].append(a-1)\n",
        "      return adj_mat, adj_list  \n",
        "# graph_read(file_name=os.path.join('/content', \"gdrive\", 'MyDrive', 'Research', 'MADRL', 'Graph Dataset', '50_499.txt'))"
      ],
      "metadata": {
        "id": "t1lzF5U5FmFN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def function_plot(data, title='loss variation', ylabel='loss', xlabel='epoch', loc=\"upper left\", fig_name=None):\n",
        "  plt.plot(data)\n",
        "  plt.title(title)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.show()\n",
        "  if fig_name is not None:\n",
        "    plt.savefig(fig_name)"
      ],
      "metadata": {
        "id": "ANJSU3-EngS_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_into_csv(file_name='data.csv', save_result=[], trim=30):\n",
        "  with open(file_name, 'w', encoding='utf-8') as f:\n",
        "    csv_writer = csv.writer(f)\n",
        "    for i in range(0, min(trim, len(save_result))):\n",
        "      csv_writer.writerow([save_result[i][0], save_result[i][1]])\n",
        "  return "
      ],
      "metadata": {
        "id": "rZ7T04vjuWve"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_VALIDATION_VIOLATION_OBSERVED = None \n",
        "data_file = '500_1499'"
      ],
      "metadata": {
        "id": "rz6hKGuCweYE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj_matrix, adj_list = graph_read(file_name=os.path.join('/content', \"gdrive\", 'MyDrive', 'Research', 'MADRL', 'Graph Dataset', data_file+'.txt'))\n",
        "adj_matrix = torch.tensor(adj_matrix)\n",
        "#controller = Controller(n_nodes = 5, in_features= 2, out_features=5, decoder_out_features=10)\n",
        "#adj_matrix = torch.unsqueeze(adj_matrix, 2)\n",
        "adj_matrix = adj_matrix.unsqueeze(-1)\n",
        "#print(adj_matrix.shape)\n",
        "#controller(adj_matrix)\n",
        "temp = adj_matrix.squeeze()\n",
        "print(temp)\n",
        "print(temp[0], temp[0].shape)\n",
        "\n",
        "run(adj_mat=adj_matrix, adj_list=adj_list,  epoch=100, threshold=0.5, d0=4, d1=2, d2=2, fig_name=os.path.join('/content', \"gdrive\", 'MyDrive', 'Research', 'MADRL', 'Graph Dataset', 'GAT_HAM_'+data_file+'.jpg'),\n",
        "    data_file_name = os.path.join('/content', \"gdrive\", 'MyDrive', 'Research', 'MADRL', 'Graph Dataset', 'GAT_HAM_'+data_file+'.csv'), patience=20)\n"
      ],
      "metadata": {
        "id": "AV0F5pEPGU8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4858bd-74d9-4af1-afe1-0e3dbcffc18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) torch.Size([500])\n",
            "do = 23 d1=12 d2=6\n",
            "epoch  0\n",
            "torch.Size([500, 500, 1])\n",
            "torch.Size([500, 500])\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "redward = -723 loss = tensor([-100915.9141], grad_fn=<AddBackward0>) violation = 776\n",
            "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0]\n",
            "epoch  1\n",
            "torch.Size([500, 500, 1])\n",
            "torch.Size([500, 500])\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "redward = -714 loss = tensor([-96342.7109], grad_fn=<AddBackward0>) violation = 785\n",
            "[0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1]\n",
            "epoch  2\n",
            "torch.Size([500, 500, 1])\n",
            "torch.Size([500, 500])\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "redward = -668 loss = tensor([-92276.3828], grad_fn=<AddBackward0>) violation = 831\n",
            "[1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
            "epoch  3\n",
            "torch.Size([500, 500, 1])\n",
            "torch.Size([500, 500])\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "m = nn.Tanh()\n",
        "input = torch.randn(2)\n",
        "print(input)\n",
        "output = m(input)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "m4baL1Mzwar3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.rand(1)\n",
        "print(A)\n",
        "A.requires_grad = True\n",
        "print(A.grad)\n",
        "\n",
        "B = torch.rand(1)\n",
        "print(B)\n",
        "B.requires_grad = True\n",
        "print(B.grad)\n",
        "\n",
        "D = torch.rand(1)\n",
        "print(D)\n",
        "D.requires_grad = True\n",
        "print(D.grad)\n",
        "\n",
        "x = max(A*D, B*D)\n",
        "print(x)\n",
        "x.required_grad = True \n",
        "#print(x.grad)\n",
        "x.backward()\n",
        "\n",
        "print(\"A \", A.grad)\n",
        "print(\"B \", B.grad)\n",
        "print(\"D \",D.grad)\n"
      ],
      "metadata": {
        "id": "DM9uZSapTwUx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}